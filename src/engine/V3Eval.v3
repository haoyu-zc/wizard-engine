// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// The {V3Eval} component contains canonical Virgil-level implementations of many Wasm operations.
// These implementations are used in the implementation of the V3 interpreter as well as during
// constant-folding in the compiler(s).
// These operations are separated from the interpreter implementation so they can be wrapped and
// used in different ways.
component V3Eval {
	// ---- i32 comparisons ------------------------------------------------
	def I32_EQZ	= u32.==(0, _);
	def I32_EQ	= u32.==;
	def I32_NE	= u32.!=;
	def I32_LT_S	= i32.<;
	def I32_LT_U	= u32.<;
	def I32_GT_S	= i32.>;
	def I32_GT_U	= u32.>;
	def I32_LE_S	= i32.<=;
	def I32_LE_U	= u32.<=;
	def I32_GE_S	= i32.>=;
	def I32_GE_U	= u32.>=;

	// ---- i64 comparisons ------------------------------------------------
	def I64_EQZ	= u64.==(0, _);
	def I64_EQ	= u64.==;
	def I64_NE	= u64.!=;
	def I64_LT_S	= i64.<;
	def I64_LT_U	= u64.<;
	def I64_GT_S	= i64.>;
	def I64_GT_U	= u64.>;
	def I64_LE_S	= i64.<=;
	def I64_LE_U	= u64.<=;
	def I64_GE_S	= i64.>=;
	def I64_GE_U	= u64.>=;

	// ---- f32 comparisons ------------------------------------------------
	def F32_EQ	= float.==;
	def F32_NE	= float.!=;
	def F32_LT	= float.<;
	def F32_GT	= float.>;
	def F32_LE	= float.<=;
	def F32_GE	= float.>=;

	// ---- f64 comparisons ------------------------------------------------
	def F64_EQ	= double.==;
	def F64_NE	= double.!=;
	def F64_LT	= double.<;
	def F64_GT	= double.>;
	def F64_LE	= double.<=;
	def F64_GE	= double.>=;

	// ---- i32 arithmetic -------------------------------------------------
	def I32_CLZ(x: u32) -> u32 {
		var count = 0u;
		if (x == 0) return 32;
		while ((x & 0x80000000u) == 0) { count++; x <<= 1; }
		return count;
	}
	def I32_CTZ(x: u32) -> u32 {
		var count = 0u;
		if (x == 0) return 32;
		while ((x & 1u) == 0) { count++; x >>= 1; }
		return count;
	}
	def I32_POPCNT(x: u32) -> u32 {
		var count = 0u;
		for (i < 32) {
			if ((x & 1) == 1) count++;
			x >>= 1;
		}
		return count;
	}
	def I32_ADD	= u32.+;
	def I32_SUB	= u32.-;
	def I32_MUL	= u32.*;
	def I32_DIV_S(x: i32, y: i32) -> (i32, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		if (y == -1 && x == int.min) return (0, TrapReason.DIV_UNREPRESENTABLE);
		return (x / y, TrapReason.NONE);
	}
	def I32_DIV_U(x: u32, y: u32) -> (u32, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (x / y, TrapReason.NONE);
	}
	def I32_REM_S(x: i32, y: i32) -> (i32, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (if (y == -1, 0, x % y), TrapReason.NONE);
	}
	def I32_REM_U(x: u32, y: u32) -> (u32, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (if(y == 1, 0, x % y), TrapReason.NONE);
	}
	def I32_AND	= u32.&;
	def I32_OR	= u32.|;
	def I32_XOR	= u32.^;
	def I32_SHL(x: i32, y: i32) -> i32 {
		return x << u5.view(y);
	}
	def I32_SHR_S(x: i32, y: i32) -> i32 {
		return x >> u5.view(y);
	}
	def I32_SHR_U(x: i32, y: i32) -> i32 {
		return x >>> u5.view(y);
	}
	def I32_ROTL(x: u32, z: u32) -> u32 {
		var y = u5.view(z);
		if (y != 0) {
			var upper = x << y;
			var lower = x >> byte.view(32) - y;
			x = upper | lower;
		}
		return x;
	}
	def I32_ROTR(x: u32, z: u32) -> u32 {
		var y = u5.view(z);
		if (y != 0) {
			var upper = x << byte.view(32) - y;
			var lower = x >> y;
			x = upper | lower;
		}
		return x;
	}

	// ---- i64 arithmetic -------------------------------------------------
	def I64_CLZ(x: u64) -> u64 {
		var count = 0u;
		if (x == 0) return 64;
		while ((x & 0x8000000000000000ul) == 0) { count++; x <<= 1; }
		return count;
	}
	def I64_CTZ(x: u64) -> u64 {
		var count = 0u;
		if (x == 0) return 64;
		while ((x & 1u) == 0) { count++; x >>= 1; }
		return count;
	}
	def I64_POPCNT(x: u64) -> u64 {
		var count = 0u;
		for (i < 64) {
			if ((x & 1) == 1) count++;
			x >>= 1;
		}
		return count;
	}
	def I64_ADD	= u64.+;
	def I64_SUB	= u64.-;
	def I64_MUL	= u64.*;
	def I64_DIV_S(x: i64, y: i64) -> (i64, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		if (y == -1 && x == long.min) return (0, TrapReason.DIV_UNREPRESENTABLE);
		return (x / y, TrapReason.NONE);
	}
	def I64_DIV_U(x: u64, y: u64) -> (u64, TrapReason) {
		if (y == 0) return(0, TrapReason.DIV_BY_ZERO);
		return (x / y, TrapReason.NONE);
	}
	def I64_REM_S(x: i64, y: i64) -> (i64, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (if(y == -1, 0, x % y), TrapReason.NONE);
	}
	def I64_REM_U(x: u64, y: u64) -> (u64, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (if(y == 1, 0, x % y), TrapReason.NONE);
	}
	def I64_AND	= u64.&;
	def I64_OR	= u64.|;
	def I64_XOR	= u64.^;
	def I64_SHL(x: u64, y: u64) -> u64 {
		return x << u6.view(y);
	}
	def I64_SHR_S(x: i64, y: i64) -> i64 {
		return x >> u6.view(y);
	}
	def I64_SHR_U(x: u64, y: u64) -> u64 {
		return x >> u6.view(y);
	}
	def I64_ROTL(x: u64, z: u64) -> u64 {
		var y = u6.view(z);
		if (y != 0) {
			var upper = x << y;
			var lower = x >> byte.view(64) - y;
			x = upper | lower;
		}
		return x;
	}
	def I64_ROTR(x: u64, z: u64) -> u64 {
		var y = u6.view(z);
		if (y != 0) {
			var upper = x << byte.view(64) - y;
			var lower = x >> y;
			x = upper | lower;
		}
		return x;
	}

	// ---- f32 arithmetic -------------------------------------------------
	def F32_ABS	= float.abs;
	def F32_NEG(a: float) -> float {
		return float.view(0x80000000u ^ u32.view(a));
	}
	def F32_CEIL(a: float) -> float {
		return canonf(float.ceil(a));
	}
	def F32_FLOOR(a: float) -> float {
		return canonf(float.floor(a));
	}
	def F32_TRUNC(a: float) -> float {
		if (a < 0f) {
			if (a > -1f) return -0f;  // handle -0
			return 0f - float.floor(0f - a);
		}
		return canonf(float.floor(a));
	}
	def F32_NEAREST	= float.round;
	def F32_SQRT	= float.sqrt;
	def F32_ADD	= float.+;
	def F32_SUB	= float.-;
	def F32_MUL	= float.*;
	def F32_DIV	= float./;
	def F32_MIN(a: float, b: float) -> float {
		if (a < b) return a;
		if (a == b) return if(b.sign == 1, b, a); // handle -0
		if (b < a) return b;
		return float.nan;
	}
	def F32_MAX(a: float, b: float) -> float {
		if (a > b) return a;
		if (a == b) return if(b.sign == 0, b, a); // handle -0
		if (b > a) return b;
		return float.nan;
	}
	def F32_COPYSIGN(a: float, b: float) -> float {
		var aa = 0x7fffffffu & u32.view(a);
		var bb = 0x80000000u & u32.view(b);
		return float.view(aa | bb);
	}

	// ---- f64 arithmetic -------------------------------------------------
	def F64_ABS	= double.abs;
	def F64_NEG(a: double) -> double {
		return double.view(0x8000000000000000uL ^ u64.view(a));
	}
	def F64_CEIL(a: double) -> double {
		return canond(double.ceil(a));
	}
	def F64_FLOOR(a: double) -> double {
		return canond(double.floor(a));
	}
	def F64_TRUNC(a: double) -> double {
		if (a < 0d) {
			if (a > -1d) return -0d;  // handle -0
			return 0d - double.floor(0d - a);
		}
		return canond(double.floor(a));
	}
	def F64_NEAREST	= double.round;
	def F64_SQRT	= double.sqrt;
	def F64_ADD	= double.+;
	def F64_SUB	= double.-;
	def F64_MUL	= double.*;
	def F64_DIV	= double./;
	def F64_MIN(a: double, b: double) -> double {
		if (a < b) return a;
		if (a == b) return if(b.sign == 1, b, a); // handle -0
		if (b < a) return b;
		return double.nan;
	}
	def F64_MAX(a: double, b: double) -> double {
		if (a > b) return a;
		if (a == b) return if(b.sign == 0, b, a); // handle -0
		if (b > a) return b;
		return double.nan;
	}
	def F64_COPYSIGN(a: double, b: double) -> double {
		var aa = 0x7fffffffffffffffuL & u64.view(a);
		var bb = 0x8000000000000000uL & u64.view(b);
		return double.view(aa | bb);
	}

	// ---- v128 arithmetic ---------------------------------------------
	def V128_NOT(a: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, (u64.max, u64.max), u64.^);
	}
	def V128_AND(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, u64.&);
	}
	def V128_OR(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, u64.|);
	}
	def V128_XOR(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, u64.^);
	}
	def V128_BITSELECT(a: (u64, u64), b: (u64, u64), c: (u64, u64)) -> (u64, u64) {
		// Equivalent to v128.or(v128.and(a, c), v128.and(b, v128.not(c))).
		var not_c = V128_NOT(c);
		var and_ac = V128_AND(a, c);
		var and_b_not_c = V128_AND(b, not_c);
		return V128_OR(and_ac, and_b_not_c);
	}
	def V128_ANDNOT(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		var not_b = V128_NOT(b);
		return V128_AND(a, not_b);
	}
	def I64X2_ADD(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, u64.+);
	}
	def I64X2_SUB(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, u64.-);
	}
	def I64X2_MUL(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, u64.*);
	}
	def I64X2_NEG(a: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2((0, 0), a, u64.-);
	}
	def I64X2_ABS(a: (u64, u64)) -> (u64, u64) {
		return do_v_v_x2(a, V128_I64_ABS);
	}
	def I64X2_EQ(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, V128_I64X2_EQ);
	}
	def I64X2_NE(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, V128_I64X2_NE);
	}
	def I64X2_LT_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, V128_I64X2_LT_S);
	}
	def I64X2_LE_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, V128_I64X2_LE_S);
	}
	def I64X2_GT_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I64X2_LT_S(b, a);
	}
	def I64X2_GE_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I64X2_LE_S(b, a);
	}
	def I32X4_ADD(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, u32.+);
	}
	def I32X4_SUB(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, u32.-);
	}
	def I32X4_MUL(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, u32.*);
	}
	def I32X4_NEG(a: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4((0, 0), a, u32.-);
	}
	def I32X4_MIN_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, V128_I32_MIN_S);
	}
	def I32X4_MIN_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, I32_MIN_U);
	}
	def I32X4_MAX_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, V128_I32_MAX_S);
	}
	def I32X4_MAX_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, I32_MAX_U);
	}
	def I32X4_ABS(a: (u64, u64)) -> (u64, u64) {
		return do_v_v_x4(a, V128_I32_ABS);
	}
	def I32X4_EQ(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, V128_I32X4_EQ);
	}
	def I32X4_NE(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, V128_I32X4_NE);
	}
	def I32X4_LT_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, V128_I32X4_LT_S);
	}
	def I32X4_LT_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, V128_I32X4_LT_U);
	}
	def I32X4_LE_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, V128_I32X4_LE_S);
	}
	def I32X4_LE_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, V128_I32X4_LE_U);
	}
	def I32X4_GT_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I32X4_LT_S(b, a);
	}
	def I32X4_GT_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I32X4_LT_U(b, a);
	}
	def I32X4_GE_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I32X4_LE_S(b, a);
	}
	def I32X4_GE_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I32X4_LE_U(b, a);
	}
	def I16X8_ADD(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, u16.+);
	}
	def I16X8_SUB(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, u16.-);
	}
	def I16X8_MUL(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, u16.*);
	}
	def I16X8_NEG(a: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8((0, 0), a, u16.-);
	}
	def I16X8_MIN_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, V128_I16_MIN_S);
	}
	def I16X8_MIN_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, I16_MIN_U);
	}
	def I16X8_MAX_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, V128_I16_MAX_S);
	}
	def I16X8_MAX_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, I16_MAX_U);
	}
	def I16X8_AVGR_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, I16_AVGR_U);
	}
	def I16X8_ABS(a: (u64, u64)) -> (u64, u64) {
		return do_v_v_x8(a, V128_I16_ABS);
	}
	def I16X8_EQ(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, V128_I16X8_EQ);
	}
	def I16X8_NE(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, V128_I16X8_NE);
	}
	def I16X8_LT_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, V128_I16X8_LT_S);
	}
	def I16X8_LT_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, V128_I16X8_LT_U);
	}
	def I16X8_LE_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, V128_I16X8_LE_S);
	}
	def I16X8_LE_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x8(a, b, V128_I16X8_LE_U);
	}
	def I16X8_GT_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I16X8_LT_S(b, a);
	}
	def I16X8_GT_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I16X8_LT_U(b, a);
	}
	def I16X8_GE_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I16X8_LE_S(b, a);
	}
	def I16X8_GE_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I16X8_LE_U(b, a);
	}
	def I8X16_ADD(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, u8.+);
	}
	def I8X16_SUB(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, u8.-);
	}
	def I8X16_NEG(a: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16((0, 0), a, u8.-);
	}
	def I8X16_MIN_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, V128_I8_MIN_S);
	}
	def I8X16_MIN_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, I8_MIN_U);
	}
	def I8X16_MAX_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, V128_I8_MAX_S);
	}
	def I8X16_MAX_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, I8_MAX_U);
	}
	def I8X16_AVGR_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, I8_AVGR_U);
	}
	def I8X16_ABS(a: (u64, u64)) -> (u64, u64) {
		return do_v_v_x16(a, V128_I8_ABS);
	}
	def I8X16_POPCNT(a: (u64, u64)) -> (u64, u64) {
		return do_v_v_x16(a, I8_POPCNT);
	}
	def I8X16_EQ(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, V128_I8X16_EQ);
	}
	def I8X16_NE(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, V128_I8X16_NE);
	}
	def I8X16_LT_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, V128_I8X16_LT_S);
	}
	def I8X16_LT_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, V128_I8X16_LT_U);
	}
	def I8X16_LE_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, V128_I8X16_LE_S);
	}
	def I8X16_LE_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x16(a, b, V128_I8X16_LE_U);
	}
	def I8X16_GT_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I8X16_LT_S(b, a);
	}
	def I8X16_GT_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I8X16_LT_U(b, a);
	}
	def I8X16_GE_S(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I8X16_LE_S(b, a);
	}
	def I8X16_GE_U(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return I8X16_LE_U(b, a);
	}
	def F32X4_ADD(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, F32_ADD_U);
	}
	def F32X4_SUB(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, F32_SUB_U);
	}
	def F32X4_MUL(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, F32_MUL_U);
	}
	def F32X4_DIV(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x4(a, b, F32_DIV_U);
	}
	def F32X4_NEG(a: (u64, u64)) -> (u64, u64) {
		return do_v_v_x4(a, F32_NEG_U);
	}
	def F32X4_SQRT(a: (u64, u64)) -> (u64, u64) {
		return do_v_v_x4(a, F32_SQRT_U);
	}
	def F64X2_ADD(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, F64_ADD_U);
	}
	def F64X2_SUB(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, F64_SUB_U);
	}
	def F64X2_MUL(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, F64_MUL_U);
	}
	def F64X2_DIV(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		return do_vv_v_x2(a, b, F64_DIV_U);
	}
	def F64X2_NEG(a: (u64, u64)) -> (u64, u64) {
		return do_v_v_x2(a, F64_NEG_U);
	}
	def F64X2_SQRT(a: (u64, u64)) -> (u64, u64) {
		return do_v_v_x2(a, F64_SQRT_U);
	}

	// ---- v128 arithmetic helpers ----------------------------------------
	private def V128_I8_MIN_S = i8_s_binop(_, _, I8_MIN_S);
	private def V128_I16_MIN_S = i16_s_binop(_, _, I16_MIN_S);
	private def V128_I32_MIN_S = i32_s_binop(_, _, I32_MIN_S);
	private def V128_I8_MAX_S = i8_s_binop(_, _, I8_MAX_S);
	private def V128_I16_MAX_S = i16_s_binop(_, _, I16_MAX_S);
	private def V128_I32_MAX_S = i32_s_binop(_, _, I32_MAX_S);
	private def V128_I8_ABS = i8_s_unop(_, I8_ABS_S);
	private def V128_I16_ABS = i16_s_unop(_, I16_ABS_S);
	private def V128_I32_ABS = i32_s_unop(_, I32_ABS_S);
	private def V128_I64_ABS = i64_s_unop(_, I64_ABS_S);
	
	private def V128_I8X16_EQ = u8_bool_binop(_, _, u8.==);
	private def V128_I8X16_NE = u8_bool_binop(_, _, u8.!=);
	private def V128_I8X16_LT_S = i8_bool_binop(_, _, i8.<);
	private def V128_I8X16_LT_U = u8_bool_binop(_, _, u8.<);
	private def V128_I8X16_LE_S = i8_bool_binop(_, _, i8.<=);
	private def V128_I8X16_LE_U = u8_bool_binop(_, _, u8.<=);
	private def V128_I16X8_EQ = u16_bool_binop(_, _, u16.==);
	private def V128_I16X8_NE = u16_bool_binop(_, _, u16.!=);
	private def V128_I16X8_LT_S = i16_bool_binop(_, _, i16.<);
	private def V128_I16X8_LT_U = u16_bool_binop(_, _, u16.<);
	private def V128_I16X8_LE_S = i16_bool_binop(_, _, i16.<=);
	private def V128_I16X8_LE_U = u16_bool_binop(_, _, u16.<=);
	private def V128_I32X4_EQ = u32_bool_binop(_, _, u32.==);
	private def V128_I32X4_NE = u32_bool_binop(_, _, u32.!=);
	private def V128_I32X4_LT_S = i32_bool_binop(_, _, i32.<);
	private def V128_I32X4_LT_U = u32_bool_binop(_, _, u32.<);
	private def V128_I32X4_LE_S = i32_bool_binop(_, _, i32.<=);
	private def V128_I32X4_LE_U = u32_bool_binop(_, _, u32.<=);
	private def V128_I64X2_EQ = u64_bool_binop(_, _, u64.==);
	private def V128_I64X2_NE = u64_bool_binop(_, _, u64.!=);
	private def V128_I64X2_LT_S = i64_bool_binop(_, _, i64.<);
	private def V128_I64X2_LE_S = i64_bool_binop(_, _, i64.<=);

	private def I8_MIN_S(a: i8, b: i8) -> i8 {
		if (a <= b) return a;
		else return b;
	}
	private def I16_MIN_S(a: i16, b: i16) -> i16 {
		if (a <= b) return a;
		else return b;
	}
	private def I32_MIN_S(a: i32, b: i32) -> i32 {
		if (a <= b) return a;
		else return b;
	}
	private def I8_MAX_S(a: i8, b: i8) -> i8 {
		if (a >= b) return a;
		else return b;
	}
	private def I16_MAX_S(a: i16, b: i16) -> i16 {
		if (a >= b) return a;
		else return b;
	}
	private def I32_MAX_S(a: i32, b: i32) -> i32 {
		if (a >= b) return a;
		else return b;
	}
	private def I8_MIN_U(a: u8, b: u8) -> u8 {
		if (a <= b) return a;
		else return b;
	}
	private def I16_MIN_U(a: u16, b: u16) -> u16 {
		if (a <= b) return a;
		else return b;
	}
	private def I32_MIN_U(a: u32, b: u32) -> u32 {
		if (a <= b) return a;
		else return b;
	}
	private def I8_MAX_U(a: u8, b: u8) -> u8 {
		if (a >= b) return a;
		else return b;
	}
	private def I16_MAX_U(a: u16, b: u16) -> u16 {
		if (a >= b) return a;
		else return b;
	}
	private def I32_MAX_U(a: u32, b: u32) -> u32 {
		if (a >= b) return a;
		else return b;
	}
	private def I8_AVGR_U(a: u8, b: u8) -> u8 {
		// rounding average
		var sum = u16.view(a) + u16.view(b);
		return u8.view((sum + 1) / 2);
	}
	private def I16_AVGR_U(a: u16, b: u16) -> u16 {
		// rounding average
		var sum = u32.view(a) + u32.view(b);
		return u16.view((sum + 1) / 2);
	}
	private def I8_ABS_S(a: i8) -> i8 {
		if (a < 0) return -a;
		else return a;
	}
	private def I16_ABS_S(a: i16) -> i16 {
		if (a < 0) return -a;
		else return a;
	}
	private def I32_ABS_S(a: i32) -> i32 {
		if (a < 0) return -a;
		else return a;
	}
	private def I64_ABS_S(a: i64) -> i64 {
		if (a < 0) return -a;
		else return a;
	}
	private def I8_POPCNT(a: u8) -> u8 {
		var count:byte = 0;
		for (i < 8) {
			if ((a & 1) == 1) count++;
			a >>= 1;
		}
		return count;
	}

	private def F32_ADD_U = float_binop(_, _, float.+);
	private def F32_SUB_U = float_binop(_, _, float.-);
	private def F32_MUL_U = float_binop(_, _, float.*);
	private def F32_DIV_U = float_binop(_, _, float./);
	private def F32_NEG_U = float_unop(_, F32_NEG);
	private def F32_SQRT_U = float_unop(_, float.sqrt);
	private def F64_ADD_U = double_binop(_, _, double.+);
	private def F64_SUB_U = double_binop(_, _, double.-);
	private def F64_MUL_U = double_binop(_, _, double.*);
	private def F64_DIV_U = double_binop(_, _, double./);
	private def F64_NEG_U = double_unop(_, F64_NEG);
	private def F64_SQRT_U = double_unop(_, double.sqrt);

	// ---- rounding and conversion ----------------------------------------
	def I32_WRAP_I64	= u32.view<u64>;
	def I32_TRUNC_F32_S	= truncF32(-2.1474839E9f, 2147483648f, i32.truncf, _);
	def I32_TRUNC_F32_U	= truncF32(-1f, 4294967296f, u32.truncf, _);
	def I32_TRUNC_F64_S	= truncF64(-2147483649d, 2147483648f, i32.truncd, _);
	def I32_TRUNC_F64_U	= truncF64(-1d, 4294967296d, u32.truncd, _);
	def I64_EXTEND_I32_S	= i64.view<i32>;
	def I64_EXTEND_I32_U	= u64.view<u32>;
	def I64_TRUNC_F32_S	= truncF32(-9.223373e18f, 9223372036854775808f, i64.truncf, _);
	def I64_TRUNC_F32_U	= truncF32(-1f, 18446744073709551616f, u64.truncf, _);
	def I64_TRUNC_F64_S	= truncF64(-9.223372036854778E18d, 9223372036854775808d, i64.truncd, _);
	def I64_TRUNC_F64_U	= truncF64(-1d, 18446744073709551616d, u64.truncd, _);
	def F32_CONVERT_I32_S	= float.roundi<i32>;
	def F32_CONVERT_I32_U	= float.roundi<u32>;
	def F32_CONVERT_I64_S	= float.roundi<i64>;
	def F32_CONVERT_I64_U	= float.roundi<u64>;
	def F32_DEMOTE_F64	= float.roundd;
	def F64_CONVERT_I32_S	= double.roundi<i32>;
	def F64_CONVERT_I32_U	= double.roundi<u32>;
	def F64_CONVERT_I64_S	= double.roundi<i64>;
	def F64_CONVERT_I64_U	= double.roundi<u64>;
	def F64_PROMOTE_F32	= double.!<float>;
	def I32_REINTERPRET_F32	= u32.view<float>;
	def I64_REINTERPRET_F64	= u64.view<double>;
	def F32_REINTERPRET_I32	= float.view<u32>;
	def F64_REINTERPRET_I64	= double.view<u64>;

	def I32_EXTEND8_S(a: i32) -> i32 {
		return i8.view(a);
	}
	def I32_EXTEND16_S(a: i32) -> i32 {
		return i16.view(a);
	}
	def I64_EXTEND8_S(a: i64) -> i64 {
		return i8.view(a);
	}
	def I64_EXTEND16_S(a: i64) -> i64 {
		return i16.view(a);
	}
	def I64_EXTEND32_S(a: i64) -> i64 {
		return i32.view(a);
	}

	// ---- private utilities ----------------------------------------------
	def truncF32<T>(min: float, max: float, trunc: float -> T, a: float) -> (T, TrapReason) {
		var d: T;
		if (a >= max) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
		if (a <= min) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
		if (!(a == a)) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
		return (trunc(a), TrapReason.NONE);
	}
	def truncF64<T>(min: double, max: double, trunc: double -> T, a: double) -> (T, TrapReason) {
		var d: T;
		if (a >= max) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
		if (a <= min) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
		if (!(a == a)) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
		return (trunc(a), TrapReason.NONE);
	}
	def signExtend(st: StorageType, val: Value) -> Value {
		match (st.packing) {
			PACKED_I8 => return Value.I32(u32.view(i8.view(Values.v_i(val))));
			PACKED_I16 => return Value.I32(u32.view(i16.view(Values.v_i(val))));
			_ => return val;
		}
	}
	def zeroExtend(st: StorageType, val: Value) -> Value {
		match (st.packing) {
			PACKED_I8 => return Value.I32(u8.view(Values.v_i(val)));
			PACKED_I16 => return Value.I32(u16.view(Values.v_i(val)));
			_ => return val;
		}
	}
	private def u8_bool_binop(a: u8, b: u8, f: (u8, u8) -> bool) -> u8 {  // Adapts a unsigned bool binop to a u8 binop
		return if (f(a, b), u8.max, u8.view(0));
	}
	private def u16_bool_binop(a: u16, b: u16, f: (u16, u16) -> bool) -> u16 {  // Adapts a unsigned bool binop to a u16 binop
		return if (f(a, b), u16.max, u16.view(0));
	}
	private def u32_bool_binop(a: u32, b: u32, f: (u32, u32) -> bool) -> u32 {  // Adapts a unsigned bool binop to a u32 binop
		return if (f(a, b), u32.max, u32.view(0));
	}
	private def u64_bool_binop(a: u64, b: u64, f: (u64, u64) -> bool) -> u64 {  // Adapts a unsigned bool binop to a u64 binop
		return if (f(a, b), u64.max, u64.view(0));
	}
	private def i8_bool_binop(a: u8, b: u8, f: (i8, i8) -> bool) -> u8 {  // Adapts a signed bool binop to a u8 binop
		return if (f(i8.view(a), i8.view(b)), u8.max, u8.view(0));
	}
	private def i16_bool_binop(a: u16, b: u16, f: (i16, i16) -> bool) -> u16 {  // Adapts a signed bool binop to a u16 binop
		return if (f(i16.view(a), i16.view(b)), u16.max, u16.view(0));
	}
	private def i32_bool_binop(a: u32, b: u32, f: (i32, i32) -> bool) -> u32 {  // Adapts a signed bool binop to a u32 binop
		return if (f(i32.view(a), i32.view(b)), u32.max, u32.view(0));
	}
	private def i64_bool_binop(a: u64, b: u64, f: (i64, i64) -> bool) -> u64 {  // Adapts a signed bool binop to a u64 binop
		return if (f(i64.view(a), i64.view(b)), u64.max, u64.view(0));
	}
	private def i8_s_binop(a: u8, b: u8, f: (i8, i8) -> i8) -> u8 {  // Adapts a signed i8 binop to a u8 binop
		return u8.view(f(i8.view(a), i8.view(b)));
	}
	private def i8_s_unop(a: u8, f: i8 -> i8) -> u8 {  // Adapts a signed i8 unop to a u8 unop
		return u8.view(f(i8.view(a)));
	}
	private def i16_s_binop(a: u16, b: u16, f: (i16, i16) -> i16) -> u16 {  // Adapts a signed i16 binop to a u16 binop
		return u16.view(f(i16.view(a), i16.view(b)));
	}
	private def i16_s_unop(a: u16, f: i16 -> i16) -> u16 {  // Adapts a signed i16 unop to a u16 unop
		return u16.view(f(i16.view(a)));
	}
	private def i32_s_binop(a: u32, b: u32, f: (i32, i32) -> i32) -> u32 {  // Adapts a signed i32 binop to a u32 binop
		return u32.view(f(i32.view(a), i32.view(b)));
	}
	private def i32_s_unop(a: u32, f: i32 -> i32) -> u32 {  // Adapts a signed i32 unop to a u32 unop
		return u32.view(f(i32.view(a)));
	}
	private def i64_s_binop(a: u64, b: u64, f: (i64, i64) -> i64) -> u64 {  // Adapts a signed i64 binop to a u64 binop
		return u64.view(f(i64.view(a), i64.view(b)));
	}
	private def i64_s_unop(a: u64, f: i64 -> i64) -> u64 {  // Adapts a signed i64 unop to a u64 unop
		return u64.view(f(i64.view(a)));
	}
	private def float_binop(a: u32, b: u32, f: (float, float) -> float) -> u32 {  // Adapts a floating point binop to a u32 binop
		return u32.view(f(float.view(a), float.view(b)));
	}
	private def float_unop(a: u32, f: float -> float) -> u32 {  // Adapts a floating point unop to a u32 unop
		return u32.view(f(float.view(a)));
	}
	private def double_binop(a: u64, b: u64, f: (double, double) -> double) -> u64 {  // Adapts a floating point binop to a u64 binop
		return u64.view(f(double.view(a), double.view(b)));
	}
	private def double_unop(a: u64, f: double -> double) -> u64 {  // Adapts a floating point unop to a u64 unop
		return u64.view(f(double.view(a)));
	}
	private def do_v_v_x2(a: (u64, u64), f: (u64) -> u64) -> (u64, u64) { // Performs a 2-lane unop
		var r0 = f(a.0);
		var r1 = f(a.1);
		return (r0, r1);
	}
	private def do_v_v_x4(a: (u64, u64), f: (u32) -> u32) -> (u64, u64) { // Performs a 4-lane unop
		var r0 = f(u32.view(a.0));
		var r1 = f(u32.view(a.0 >> 32));
		var r2 = f(u32.view(a.1));
		var r3 = f(u32.view(a.1 >> 32));
		return ((u64.view(r1) << 32) | r0, (u64.view(r3) << 32) | r2);
	}
	private def do_v_v_x8(a: (u64, u64), f: (u16) -> u16) -> (u64, u64) { // Performs an 8-lane unop
		var low: u64 = 0;
		var high: u64 = 0;

		for (shift: byte = 0; shift < 64; shift += 16) {
			var r_a = u16.view((a.0 >> shift) & 0xFFFF);
			var res = f(r_a);
			low |= (u64.view(res) << shift);
			
			r_a = u16.view((a.1 >> shift) & 0xFFFF);
			res = f(r_a);
			high |= (u64.view(res) << shift);
		}

		return (low, high);
	}
	private def do_v_v_x16(a: (u64, u64), f: (u8) -> u8) -> (u64, u64) { // Performs a 16-lane unop
		var low: u64 = 0;
		var high: u64 = 0;

		for (shift: byte = 0; shift < 64; shift += 8) {
			var r_a = u8.view((a.0 >> shift) & 0xFF);
			var res = f(r_a);
			low |= (u64.view(res) << shift);
			
			r_a = u8.view((a.1 >> shift) & 0xFF);
			res = f(r_a);
			high |= (u64.view(res) << shift);
		}

		return (low, high);
	}
	private def do_vv_v_x2(a: (u64, u64), b: (u64, u64), f: (u64, u64) -> u64) -> (u64, u64) { // Performs a 2-lane binop
		var r0 = f(a.0, b.0);
		var r1 = f(a.1, b.1);
		return (r0, r1);
	}
	private def do_vv_v_x4(a: (u64, u64), b: (u64, u64), f: (u32, u32) -> u32) -> (u64, u64) { // Performs a 4-lane binop
		var r0 = f(u32.view(a.0), u32.view(b.0));
		var r1 = f(u32.view(a.0 >> 32), u32.view(b.0 >> 32));
		var r2 = f(u32.view(a.1), u32.view(b.1));
		var r3 = f(u32.view(a.1 >> 32), u32.view(b.1 >> 32));
		return ((u64.view(r1) << 32) | r0, (u64.view(r3) << 32) | r2);
	}
	private def do_vv_v_x8(a: (u64, u64), b: (u64, u64), f: (u16, u16) -> u16) -> (u64, u64) { // Performs an 8-lane binop
		var low: u64 = 0;
		var high: u64 = 0;

		for (shift: byte = 0; shift < 64; shift += 16) {
			var r_a = u16.view((a.0 >> shift) & 0xFFFF);
			var r_b = u16.view((b.0 >> shift) & 0xFFFF);
			var res = f(r_a, r_b);
			low |= (u64.view(res) << shift);
			
			r_a = u16.view((a.1 >> shift) & 0xFFFF);
			r_b = u16.view((b.1 >> shift) & 0xFFFF);
			res = f(r_a, r_b);
			high |= (u64.view(res) << shift);
		}

		return (low, high);
	}
	private def do_vv_v_x16(a: (u64, u64), b: (u64, u64), f: (u8, u8) -> u8) -> (u64, u64) { // Performs a 16-lane binop
		var low: u64 = 0;
		var high: u64 = 0;

		for (shift: byte = 0; shift < 64; shift += 8) {
			var r_a = u8.view((a.0 >> shift) & 0xFF);
			var r_b = u8.view((b.0 >> shift) & 0xFF);
			var res = f(r_a, r_b);
			low |= (u64.view(res) << shift);
			
			r_a = u8.view((a.1 >> shift) & 0xFF);
			r_b = u8.view((b.1 >> shift) & 0xFF);
			res = f(r_a, r_b);
			high |= (u64.view(res) << shift);
		}

		return (low, high);
	}
	private def canonf(a: float) -> float {
		return if(a == a, a, float.nan);
	}
	private def canond(a: double) -> double {
		return if(a == a, a, double.nan);
	}
}
